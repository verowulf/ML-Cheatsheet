{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ML Cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dt_bh = datasets.load_boston()    # Boston house price ; (506, 13)\n",
    "dt_ir = datasets.load_iris()      # Iris 0, 1, 2 ; (150, 4) flowers. The latter [50:150] more difficult\n",
    "dt_dg = datasets.load_digits()    # Digits 0, 1, 2, ..., 9 ; (1797, 64) 8x8 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Regression (Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = dt_bh.data\n",
    "y = dt_bh.target\n",
    "\n",
    "# Train test split\n",
    "X_tn, X_tt, y_tn, y_tt = model_selection.train_test_split(X, y, test_size=0.3, random_state=27)\n",
    "\n",
    "# Normalization\n",
    "ss = preprocessing.StandardScaler()\n",
    "print(ss)\n",
    "\n",
    "X_tn = ss.fit_transform(X_tn)\n",
    "X_tt = ss.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "rgs_lin = LinearRegression()\n",
    "print(rgs_lin)\n",
    "\n",
    "rgs_lin.fit(X_tn, y_tn)\n",
    "print(rgs_lin.score(X_tn, y_tn))\n",
    "print(rgs_lin.score(X_tt, y_tt))\n",
    "\n",
    "#rgs_lin.predict(X_tt)\n",
    "#print(rgs_lin.coef_)\n",
    "#print(rgs_lin.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classification (Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X = dt_dg.data\n",
    "#y = dt_dg.target\n",
    "\n",
    "X = dt_ir.data[:]\n",
    "y = dt_ir.target[:]\n",
    "\n",
    "# Train test split\n",
    "X_tn, X_tt, y_tn, y_tt = model_selection.train_test_split(X, y, test_size=0.3, random_state=27)\n",
    "\n",
    "# Normalization\n",
    "ss = preprocessing.StandardScaler()\n",
    "\n",
    "X_tn = ss.fit_transform(X_tn)\n",
    "X_tt = ss.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(random_state=27)\n",
    "print(clf_lr)\n",
    "\n",
    "clf_lr.fit(X_tn, y_tn)\n",
    "print(clf_lr.score(X_tn, y_tn))\n",
    "print(clf_lr.score(X_tt, y_tt))\n",
    "\n",
    "#print(clf_lr.coef_)\n",
    "#print(clf_lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# kNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5, p=2)\n",
    "print(clf_knn)\n",
    "\n",
    "clf_knn.fit(X_tn, y_tn)\n",
    "print(clf_knn.score(X_tn, y_tn))\n",
    "print(clf_knn.score(X_tt, y_tt))\n",
    "\n",
    "#np.column_stack((clf_knn.predict(X_tt), clf_knn.predict_proba(X_tt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC(random_state=27)\n",
    "print(clf_svc)\n",
    "\n",
    "clf_svc.fit(X_tn, y_tn)\n",
    "print(clf_svc.score(X_tn, y_tn))\n",
    "print(clf_svc.score(X_tt, y_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB    # or BernoulliNB\n",
    "clf_nb = GaussianNB()\n",
    "print(clf_nb)\n",
    "\n",
    "clf_nb.fit(X_tn, y_tn)\n",
    "print(clf_nb.score(X_tn, y_tn))\n",
    "print(clf_nb.score(X_tt, y_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt = DecisionTreeClassifier(criterion='gini', random_state=27)    # or criterion='entropy' (info gain)\n",
    "print(clf_dt)\n",
    "\n",
    "clf_dt.fit(X_tn, y_tn)    # Conditions are identified\n",
    "print(clf_dt.score(X_tn, y_tn))\n",
    "print(clf_dt.score(X_tt, y_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Random Forest: Collection of decision trees that use a random subset of training data(Bagging) and features --> majority vote\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(n_estimators=10, random_state=27)    # n_estimators: num of trees (higher if num of features higher)\n",
    "print(clf_rf)\n",
    "\n",
    "clf_rf.fit(X_tn, y_tn)    # Conditions are identified\n",
    "print(clf_rf.score(X_tn, y_tn))\n",
    "print(clf_rf.score(X_tt, y_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting & Ada Boost\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf_gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=5, random_state=27)\n",
    "print(clf_gb)\n",
    "\n",
    "clf_gb.fit(X_tn, y_tn)\n",
    "print(clf_gb.score(X_tn, y_tn))\n",
    "print(clf_gb.score(X_tt, y_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Tuning hyper-parameters for accuracy\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "0.962 (+/-0.099) for {'n_neighbors': 3, 'p': 1}\n",
      "0.943 (+/-0.149) for {'n_neighbors': 3, 'p': 2}\n",
      "0.962 (+/-0.099) for {'n_neighbors': 5, 'p': 1}\n",
      "0.933 (+/-0.177) for {'n_neighbors': 5, 'p': 2}\n",
      "0.971 (+/-0.080) for {'n_neighbors': 10, 'p': 1}\n",
      "0.971 (+/-0.080) for {'n_neighbors': 10, 'p': 2}\n",
      "0.962 (+/-0.075) for {'n_neighbors': 15, 'p': 1}\n",
      "0.962 (+/-0.073) for {'n_neighbors': 15, 'p': 2}\n",
      "\n",
      "# Best parameters on development set: {'n_neighbors': 10, 'p': 1}\n",
      "\n",
      "# Scores computed on evaluation set:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      1.000     1.000     1.000        12\n",
      "          1      0.875     0.875     0.875        16\n",
      "          2      0.882     0.882     0.882        17\n",
      "\n",
      "avg / total      0.911     0.911     0.911        45\n",
      "\n",
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'),\n",
      "       fit_params={}, iid=True, n_jobs=4,\n",
      "       param_grid=[{'n_neighbors': [3, 5, 10, 15], 'p': [1, 2]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring='accuracy', verbose=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  40 out of  40 | elapsed:    5.1s finished\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV : Exhaustive search of hyper-parameters for an estimator\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = clf_knn\n",
    "param_grid = [{'n_neighbors': [3, 5, 10, 15], 'p': [1, 2]}]\n",
    "scorings = ['accuracy']    # 'accuracy', 'precision', 'recall', 'f1'\n",
    "\n",
    "for scoring in scorings:\n",
    "    print('\\n# Tuning hyper-parameters for %s' % scoring)\n",
    "    gcv = model_selection.GridSearchCV(model, param_grid, scoring, cv=5, n_jobs=4, verbose=1)\n",
    "    gcv.fit(X_tn, y_tn)\n",
    "\n",
    "    means = gcv.cv_results_['mean_test_score']\n",
    "    stds  = gcv.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, gcv.cv_results_['params']):\n",
    "        print('%.3f (+/-%.3f) for %r' % (mean, std * 2, params))\n",
    "    print('\\n# Best parameters on development set:', gcv.best_params_)\n",
    "\n",
    "    print('\\n# Scores computed on evaluation set:\\n')\n",
    "    print(classification_report(y_tt, gcv.predict(X_tt), digits=3))\n",
    "\n",
    "print(gcv)\n",
    "#print(gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Clustering (Unsupervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = dt_ir.data[:100]    # The first 100 are easier\n",
    "y = dt_ir.target[:100]\n",
    "\n",
    "# Train test split\n",
    "X_tn, X_tt, y_tn, y_tt = model_selection.train_test_split(X, y, test_size=0.3, random_state=27)\n",
    "\n",
    "# Normalization\n",
    "ss = preprocessing.StandardScaler()\n",
    "print(ss)\n",
    "\n",
    "X_tn = ss.fit_transform(X_tn)\n",
    "X_tt = ss.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# k-Means\n",
    "from sklearn.cluster import KMeans\n",
    "clu_km = KMeans(n_clusters=2, random_state=27)\n",
    "print(clu_km)\n",
    "\n",
    "clu_km.fit(X_tn)\n",
    "clu_km.predict(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_tt    # Prediction above should cluster similarly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dimensionality Reduction (Unsupervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = dt_ir.data\n",
    "y = dt_ir.target\n",
    "\n",
    "# Train test split\n",
    "X_tn, X_tt, y_tn, y_tt = model_selection.train_test_split(X, y, test_size=0.3, random_state=27)\n",
    "\n",
    "# Normalization\n",
    "ss = preprocessing.StandardScaler()\n",
    "print(ss)\n",
    "\n",
    "X_tn = ss.fit_transform(X_tn)\n",
    "X_tt = ss.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "dr_pca = PCA(n_components=3, random_state=27)\n",
    "print(dr_pca)\n",
    "\n",
    "X_tn_reduced = dr_pca.fit_transform(X_tn)\n",
    "X_tt_reduced = dr_pca.transform(X_tt)\n",
    "\n",
    "X_tt_reduced.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
