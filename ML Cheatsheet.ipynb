{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ML Cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dt_bh = datasets.load_boston()    # Boston house price (506, 13)\n",
    "dt_ir = datasets.load_iris()      # Iris {0, 1, 2} (150, 4) flowers. The latter [50:150] more difficult\n",
    "dt_dg = datasets.load_digits()    # Digits {0, 1, ..., 9} (1797, 64) 8x8 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Demonstrate the Supervised Learning model\n",
    "def DemoSL(model):\n",
    "    print(model)\n",
    "    model.fit(X_tn, y_tn)\n",
    "    print(model.score(X_tn, y_tn))\n",
    "    print(model.score(X_tt, y_tt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### I.1. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n"
     ]
    }
   ],
   "source": [
    "X = dt_bh.data\n",
    "y = dt_bh.target\n",
    "\n",
    "# Train test split\n",
    "X_tn, X_tt, y_tn, y_tt = model_selection.train_test_split(X, y, test_size=100, random_state=27)\n",
    "\n",
    "# Normalization\n",
    "ss = preprocessing.StandardScaler()\n",
    "X_tn = ss.fit_transform(X_tn)\n",
    "print(ss)\n",
    "\n",
    "X_tt = ss.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "0.747301733399\n",
      "0.70416565463\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "rgs_lin = LinearRegression()\n",
    "DemoSL(rgs_lin)\n",
    "# Note that the score is not accuracy (percentage)\n",
    "\n",
    "#rgs_lin.predict(X_tt)\n",
    "#print(rgs_lin.coef_)\n",
    "#print(rgs_lin.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### I.2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dt_dg.data\n",
    "y = dt_dg.target\n",
    "\n",
    "# Train test split\n",
    "X_tn, X_tt, y_tn, y_tt = model_selection.train_test_split(X, y, test_size=0.3, random_state=27)\n",
    "\n",
    "# Normalization\n",
    "ss = preprocessing.StandardScaler()\n",
    "X_tn = ss.fit_transform(X_tn)\n",
    "\n",
    "X_tt = ss.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GridSearchCV : Exhaustive search of hyper-parameters for an estimator\n",
    "def gsCV(model, param_grid, scorings):\n",
    "\n",
    "    for scoring in scorings:\n",
    "        print('\\n# Tuning hyper-parameters for %s' % scoring)\n",
    "        gcv = model_selection.GridSearchCV(model, param_grid, scoring, cv=5, n_jobs=4, verbose=1)\n",
    "        gcv.fit(X_tn, y_tn)\n",
    "\n",
    "        means = gcv.cv_results_['mean_test_score']\n",
    "        stds  = gcv.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, gcv.cv_results_['params']):\n",
    "            print('%.3f (+/-%.3f) for %r' % (mean, std * 2, params))\n",
    "        print('\\n# Best parameters on development set:', gcv.best_params_)\n",
    "\n",
    "        print('\\n# Scores computed on evaluation set:\\n')\n",
    "        print(metrics.classification_report(y_tt, gcv.predict(X_tt), digits=3))\n",
    "\n",
    "    print(gcv)\n",
    "    #print(gcv.cv_results_)\n",
    "\n",
    "scorings = ['accuracy']    # 'accuracy', 'precision', 'recall', 'f1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=27, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.989657915672\n",
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_log = LogisticRegression(random_state=27)\n",
    "DemoSL(clf_log)\n",
    "\n",
    "#print(clf_log.coef_)\n",
    "#print(clf_log.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "0.982498011138\n",
      "0.975925925926\n"
     ]
    }
   ],
   "source": [
    "# kNN (Instance-based learning)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5, p=2)\n",
    "DemoSL(clf_knn)\n",
    "\n",
    "#np.column_stack((clf_knn.predict(X_tt), np.round(clf_knn.predict_proba(X_tt), 3)))\n",
    "\n",
    "param_grid = [{'n_neighbors': [3, 5, 10, 15], 'p': [1, 2]}]\n",
    "#gsCV(clf_knn, param_grid, scorings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.03, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=27, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "0.998408910103\n",
      "0.985185185185\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC(kernel='rbf', gamma=0.03, C=1, random_state=27)    # gamma for 'rbf', 'poly', 'sigmoid'\n",
    "DemoSL(clf_svc)\n",
    "\n",
    "param_grid = [{'kernel': ['rbf', 'linear', 'poly'], 'C': [3, 10, 20]}]\n",
    "#gsCV(clf_svc, param_grid, scorings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 50, 20), learning_rate='constant',\n",
      "       learning_rate_init=0.05, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=27, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
      "       warm_start=False)\n",
      "Iteration 1, loss = 1.78878120\n",
      "Iteration 2, loss = 0.66641602\n",
      "Iteration 3, loss = 0.40429631\n",
      "Iteration 4, loss = 0.25460711\n",
      "Iteration 5, loss = 0.23522668\n",
      "Iteration 6, loss = 0.21030993\n",
      "Iteration 7, loss = 0.17143242\n",
      "Iteration 8, loss = 0.18908945\n",
      "Iteration 9, loss = 0.22671110\n",
      "Iteration 10, loss = 0.19936339\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "0.989657915672\n",
      "0.959259259259\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf_mlp = MLPClassifier((100, 50, 20), learning_rate_init=0.05, alpha=0.05, verbose=1, random_state=27)\n",
    "DemoSL(clf_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "0.826571201273\n",
      "0.824074074074\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB    # or BernoulliNB\n",
    "clf_nb = GaussianNB()\n",
    "DemoSL(clf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gaussian Process\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "clf_gp = GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True, max_iter_predict=2, n_jobs=4, random_state=27)\n",
    "#DemoSL(clf_gp)    # Commented out because it takes too long for dt_dg. It is quick for dt_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
      "               store_covariances=False, tol=0.0001)\n",
      "0.936356404137\n",
      "0.833333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luke computer\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:695: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# QDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "clf_qda = QuadraticDiscriminantAnalysis()\n",
    "DemoSL(clf_qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=27, splitter='best')\n",
      "0.965791567224\n",
      "0.874074074074\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt = DecisionTreeClassifier(max_depth=7, criterion='entropy', random_state=27)    # criterion='gini' or 'entropy' (info gain)\n",
    "DemoSL(clf_dt)\n",
    "\n",
    "param_grid = [{'max_depth': [10, 20, 40], 'max_features': [32, None], 'criterion': ['gini', 'entropy']}]\n",
    "#gsCV(clf_dt, param_grid, scorings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=300, n_jobs=1, oob_score=False, random_state=27,\n",
      "            verbose=0, warm_start=False)\n",
      "1.0\n",
      "0.977777777778\n"
     ]
    }
   ],
   "source": [
    "# Random Forest: Collection of decision trees that use a random subset of training data(Bagging) and features --> majority vote\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=27)\n",
    "DemoSL(clf_rf)\n",
    "\n",
    "param_grid = [{'max_depth': [7, 10, 15], 'n_estimators': [50, 100, 300]}]\n",
    "#gsCV(clf_rf, param_grid, scorings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.01, n_estimators=300, random_state=27)\n",
      "0.768496420048\n",
      "0.733333333333\n"
     ]
    }
   ],
   "source": [
    "# Ada Boost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf_ab = AdaBoostClassifier(n_estimators=300, learning_rate=0.01, random_state=27)\n",
    "DemoSL(clf_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.5, loss='deviance', max_depth=5,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=100, presort='auto', random_state=27,\n",
      "              subsample=1.0, verbose=0, warm_start=False)\n",
      "1.0\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf_gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=5, random_state=27)\n",
    "DemoSL(clf_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### II.1. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = dt_ir.data[:100]    # The first 100 are easier\n",
    "y = dt_ir.target[:100]\n",
    "\n",
    "# Train test split\n",
    "X_tn, X_tt, y_tn, y_tt = model_selection.train_test_split(X, y, test_size=0.3, random_state=27)\n",
    "\n",
    "# Normalization\n",
    "ss = preprocessing.StandardScaler()\n",
    "X_tn = ss.fit_transform(X_tn)\n",
    "\n",
    "X_tt = ss.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=27, tol=0.0001, verbose=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k-Means\n",
    "from sklearn.cluster import KMeans\n",
    "clu_km = KMeans(n_clusters=2, random_state=27)\n",
    "print(clu_km)\n",
    "\n",
    "clu_km.fit(X_tn)\n",
    "clu_km.predict(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tt    # Prediction above should cluster similarly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### II.2. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = dt_ir.data\n",
    "y = dt_ir.target\n",
    "\n",
    "# Train test split\n",
    "X_tn, X_tt, y_tn, y_tt = model_selection.train_test_split(X, y, test_size=0.3, random_state=27)\n",
    "\n",
    "# Normalization\n",
    "ss = preprocessing.StandardScaler()\n",
    "X_tn = ss.fit_transform(X_tn)\n",
    "\n",
    "X_tt = ss.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(copy=True, iterated_power='auto', n_components=3, random_state=27,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "dr_pca = PCA(n_components=3, random_state=27)\n",
    "print(dr_pca)\n",
    "\n",
    "X_tn_reduced = dr_pca.fit_transform(X_tn)\n",
    "X_tt_reduced = dr_pca.transform(X_tt)\n",
    "\n",
    "X_tt_reduced.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
